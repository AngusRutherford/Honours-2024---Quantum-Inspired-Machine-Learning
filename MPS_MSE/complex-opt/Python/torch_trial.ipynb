{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import sympy as smp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(x):\n",
    "    # convert x to a pytorch tensor\n",
    "    x_tensor = torch.tensor(x, dtype=torch.cfloat)\n",
    "\n",
    "    # calculate the two components of the feature map, s1 and s2\n",
    "    s1 = torch.exp(1j * (3*torch.pi/2) * x_tensor) * torch.cos(torch.pi/2 * x_tensor)\n",
    "    s2 = torch.exp(-1j * (3*torch.pi/2) * x_tensor) * torch.sin(torch.pi/2 * x_tensor)\n",
    "\n",
    "    feature_vector = torch.stack((s1, s2), dim=-1)\n",
    "\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feature_map(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View vector as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25000003+0.76942086j, -0.18163565-0.55901694j], dtype=complex64)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the output is normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999403953552"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].abs().square().add(out[1].abs().square()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now that we have the feature map, let's make our complex valued MPS. To keep things simple, we will consider the weight MPS for class A and class B separately. Our MPS has two sites and a local dimension of 2, so the first site has dimension $(1, d, \\chi)$ and the second has dimension $(\\chi, d, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi = 4\n",
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "t1 = torch.randn(1, d, chi, dtype=torch.cfloat, requires_grad=True)\n",
    "t2 = torch.randn(chi, d, 1, dtype=torch.cfloat, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect the elements of the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.796089  -0.8148417j , -0.17718582-0.30679867j,\n",
       "          0.6001288 +0.4893244j , -0.22345477-1.495686j  ],\n",
       "        [ 0.22788277-0.8933126j ,  0.24747548+0.21788357j,\n",
       "          0.08474074+0.8751563j ,  0.7896807 -0.1748517j ]]],\n",
       "      dtype=complex64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9564706 -1.1992046j ],\n",
       "        [ 0.40068242+0.5610952j ]],\n",
       "\n",
       "       [[ 0.42344344-1.0996182j ],\n",
       "        [-0.24137817+1.3102732j ]],\n",
       "\n",
       "       [[ 0.530464  -0.414009j  ],\n",
       "        [-0.1226102 +0.12973848j]],\n",
       "\n",
       "       [[ 0.9824302 +1.1217078j ],\n",
       "        [ 0.66913396-0.5965696j ]]], dtype=complex64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's take a random product state and try and contract it with our 2 site MPS. We start by contracting the product state with the first MPS tensor t1 and then contract the state vector corresponding to the second site with t2. Then we contract the resulting tensors from these two operations over the shared bond dimension $\\chi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = feature_map(0.4) # construct our 2 product states\n",
    "p2 = feature_map(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_1 = torch.einsum('ijk,j -> ik', t1, p1) # contract t1 with product state 1\n",
    "operation_2 = torch.einsum('ijk,j -> ik', t2, p2)# contract t2 with product state 2\n",
    "result = torch.einsum('ij, jk -> ik', operation_1, operation_2) # contract over the shared bond dimension chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be rank 1 (scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24425482749938965+0.5001320242881775j)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert this to our final output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5565900206565857"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.abs().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our overlap $|f(x)|$. Now let's pretend that these product states correspond to a class 1 sample. Let's get the loss and gradient, then backprop using autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretend that the target is 1 ie. overlap is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(1.0, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5566, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.squeeze() # squeeze to remove extra dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_func(result.squeeze(), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the gradients for both tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6079-0.8438j, -0.7655+0.1631j, -0.3822+0.2700j,  0.4340+0.8747j],\n",
       "         [-0.0031+0.7556j,  0.5196+0.2310j,  0.3400+0.0045j,  0.1185-0.6995j]]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0026-0.4119j],\n",
       "         [ 0.0525-0.0387j]],\n",
       "\n",
       "        [[-0.1248-0.3544j],\n",
       "         [ 0.0338-0.0490j]],\n",
       "\n",
       "        [[-0.0332+0.1209j],\n",
       "         [-0.0186+0.0070j]],\n",
       "\n",
       "        [[-0.5663-0.6539j],\n",
       "         [ 0.0311-0.1334j]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually update the two tensors. We'll use a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6079-0.8438j, -0.7655+0.1631j, -0.3822+0.2700j,  0.4340+0.8747j],\n",
       "         [-0.0031+0.7556j,  0.5196+0.2310j,  0.3400+0.0045j,  0.1185-0.6995j]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7961-0.8148j, -0.1772-0.3068j,  0.6001+0.4893j, -0.2235-1.4957j],\n",
       "         [ 0.2279-0.8933j,  0.2475+0.2179j,  0.0847+0.8752j,  0.7897-0.1749j]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t1.copy_(t1 - lr * t1.grad)\n",
    "    t1.grad = None\n",
    "    t2.copy_(t2 - lr * t2.grad)\n",
    "    t2.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7657-0.7727j, -0.1389-0.3150j,  0.6192+0.4758j, -0.2452-1.5394j],\n",
       "         [ 0.2280-0.9311j,  0.2215+0.2063j,  0.0677+0.8749j,  0.7838-0.1399j]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have updated both tensors, let's re-compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_1 = torch.einsum('ijk,j -> ik', t1, p1) # contract t1 with product state 1\n",
    "operation_2 = torch.einsum('ijk,j -> ik', t2, p2) # contract t2 with product state 2\n",
    "result = torch.einsum('ij, jk -> ik', operation_1, operation_2) # contract over the shared bond dimension chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8667, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.abs().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale up to a larger example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
